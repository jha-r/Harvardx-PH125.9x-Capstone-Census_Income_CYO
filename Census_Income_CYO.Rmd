---
title: "HavardX PH125.9x Data Science Capstone CYO Project Report"
author: "Ravi Jha"
date: "12/20/2019"
output:
  html_document: default
  pdf_document:
    df_print: kable
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
    library(knitr)
    knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                          cache.lazy = FALSE, fig.align='center', fig.width=5.5, fig.height=3.5)
```

\pagebreak

# 1. Introduction

### 1.1 Overview

This report is prepared to fulfill the completion requirement of the **HavardX PH125.9x Data Science Capstone** course. 
TBD

model to classify people using demographics to predict whether a person will have an annual income over 50K dollars or not.

### 1.2 Motivation
TBD

### 1.3 Project Goal

The goal of this project is to train machine learning algorithms that predicts whether income exceeds $50K/yr based on census data and then compare their performance.


### 1.4 Key steps performed

Following are the key steps performed in this project:

 * Importing the dataset and setup
 * Spliting datasets
 * Data wrangling
 * Exploratory data analysis and visualization 
 * Creating and tuning predective models
 * Evaluating models based on validation dataset
 * Reporting results 

\pagebreak

# 2. Data and setup

The first step in the whole process is to import and setup data including splitting it in appropriate sub-datasets. 

### 2.1 Adult Census Income DataSet

Let's start with the dataset, as per course instruction, Machine Learning analyses friendly **Adult Census Income** dataset, from UCI's curated list of datasets, is used. The data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). The dataset is then cleaned considering the following criteria: 

 - age > 16
 - adjusted gross income > $100
 - final weighting > 1
 - working hours per week > 0

This data set can be found and downloaded here:

* Adult Census Income dataset:           http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data


### 2.2 Data Import and Initial Setup

To assist with our data import and initial setup, several packages from *CRAN* is utilized and loaded. These will be automatically downloaded and installed during code execution. 


```{r data import and initial setup}
##########################
# to import and setup data
##########################
# Install requested packages if not found
if (!require(tidyverse))
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if (!require(caret))
  install.packages("caret", repos = "http://cran.us.r-project.org")
if (!require(data.table))
  install.packages("data.table", repos = "http://cran.us.r-project.org")
if (!require(dplyr))
  install.packages("dplyr")
if (!require(tidyr))
  install.packages("tidyr")
# Load libraries
library(tidyverse)
library(caret)
library(hexbin)
library(lubridate)
library(knitr)
library(kableExtra)
# Adult Census Income dataset
# http://archive.ics.uci.edu/ml/machine-learning-databases/adult
datafile <- "Income.RData"
# Check if datafile is already downloaded
if (!file.exists("Income.RData"))
{
  fileURL <-
    "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
  
  destfile_adult <- "./dataset/adult.data"
  
  # Download only if destination data file - adult.data
  # is not found on the dataset directory folder
  if (!file.exists(destfile_adult))
  {
    # Create dataset folder file if it doesn't exists
    if (!dir.exists("./dataset"))
    {
      dir.create("./dataset")
    }
    
    # Download file from url to temp file
    download.file(fileURL, destfile_adult,  quiet = TRUE)
    print("Downloading file to the dataset directory")
    
  }
  
  c_names <-
    c(
      "age", "workclass", "fnlwgt", "education", "education-num", 
      "marital-status", "occupation", "relationship", "race", "sex", 
      "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"
    )
  
  
  # read the data from the file and store in the census_data object
  census_data <-
    read.csv(file = "./dataset/adult.data", header = FALSE, 
             col.names = c_names, strip.white = TRUE)
  
  # replace . in the columns names
  colnames(census_data) <- str_replace_all(colnames(census_data), "\\.", "_")
  
  # Save census_data object to datafile
  save(census_data, file = datafile)
  
  # Remove unused objects from the memory
  rm(c_names, fileURL, datafile, destfile_adult)
  
} else {
  # Load the datafile if it already exists
  load(datafile)
}
```



<br/>

\pagebreak

# 3. Data Wrangling

The next step is to understand the data through a brief data review and then do data wrangling as per the necessity.

### 3.1 Initial exploration

Before proceeding further with analysis, examining the data is important. The first step in it is to understand the format and the contents by looking at the few rows.

```{r Inital analysis-head}
    # Display first few rows to understand the data structure
    head(census_data, 5)%>%
    
    # to format table with theme
    kable() %>% 
    kable_styling(latex_options = c("striped", "hover", "condensed", "scale_down"))
```


<br/>

The dataset has 32,561 records and 15 attributes

```{r row_columns class, echo=FALSE}
    
    # to display class of userId and movieId column in a tabular form
      # to display number of rows and columns in a tabular form
      row_cols <- tibble(Rows = nrow(census_data), 
                             Columns = ncol(census_data))
      row_cols %>% 
      
      # to apply theme to the table
      kable() %>% 
      kable_styling(latex_options = c("striped", "hover", "condensed"))
``` 

<br/>


#### Dataset Summary 

```{r summary, include=TRUE}
    # Summary of the census_data_train dataset
    summary(census_data)%>%
    
    kable() %>% 
    kable_styling(latex_options = c("striped", "hover", "condensed", "scale_down"))
```

<br/>


#### Column types and class

Let's check column types and their class


```{r Columns}
    # To display dataset column types and summary
    col_type <- as.data.frame(lapply(census_data, class)) %>%
      .[1, ] %>%
      gather(variable, R_class , 1:ncol(.)) %>%
      mutate(data_type = ifelse(
        R_class == "factor",
        "categorical",
        ifelse(variable == "education_num", "categorical", "continuous")
      )) 

        col_type%>%
      
      # to format table with theme
      kable() %>%
      kable_styling(latex_options = c("striped", "hover", "condensed")) 
```


<br/>

#### Duplicate Records

Let's first check for any duplicate record. The following table shows there are 24 duplicate rows.

```{r duplicates}
    census_data %>%
      summarize(no_of_observations = n(),
                no_of_distinct = n_distinct(.)) %>%
      mutate(no_of_dup = no_of_observations - no_of_distinct) %>%
      mutate(pct_dup = round(no_of_dup / no_of_observations * 100, 1)) %>%
      # to format table with theme
      kable() %>%
      kable_styling(latex_options = c("striped", "hover", "condensed")) 

```


<br/>

#### Missing Data

By closely looking at the data, it can be observed that there are columns with value "?". This is also supported by the dataset details available on UCI wesite that **unknown** values are labelled as **?**. 

As it can be seen from the below table, there are 2,399 incomplete records (~7.4%). Specifically, there are missing values in occupation, workclass and native_country, which accounts for 5.7%, 5.6% and 1.8% of the total records respectively.

```{r missing data}
    #to check if any missing value marked with NA
      anyNA(census_data)
    
    # to check missing values labeled as "?"
    total_missing <-
      map_df(census_data, ~ str_detect(., pattern = "\\?")) %>%
      rowSums() %>%
      tbl_df() %>%
      filter(value > 0) %>%
      summarize(no_of_missing = n()) %>%
      filter(no_of_missing > 1) %>%
      mutate(pct_missing = round(no_of_missing / nrow(census_data) * 100, 1),
             variable = "all variables") %>%

      select(variable, no_of_missing, pct_missing)
    
    
    map_df(census_data, ~ sum(str_detect(., pattern = "\\?"))) %>%

      gather(variable, no_of_missing, 1:15) %>%
      mutate(pct_missing = round(no_of_missing / nrow(census_data) * 100, 1)) %>%
      bind_rows(total_missing) %>%
      filter(no_of_missing > 1) %>%
      arrange(desc(no_of_missing)) %>%
      # to format table with theme
      kable() %>%
      kable_styling(latex_options = c("striped", "hover", "condensed")) %>%
      row_spec(1:1, color = "white", background = "#D7261E")
```


<br/>


<br/>

#### Factorizing Variables
# TBD
In the dataset, columns `userId` and `movieId` contains numeric data and treated as numeric/integer by R. These columns can be converted to factor as this could be used as categorical data; however, in the current scenario it makes more sense to leave it as numeric to keep later computation easier e.g. summarize() function and other numeric calculations can be done easily.

```{r userid-movieid class}
    # to factoring variables to exclude the unwanted levels
    census_data$workclass <- factor(census_data$workclass)
    census_data$occupation <- factor(census_data$occupation)
    census_data$native_country <- factor(census_data$native_country)

``` 


<br/>

#### Label encoding
There are no columns with content as ordinal labels, so label encoding is not required in the dataset.


<br/>

### 3.2 Tidying data

Based on the initial understanding further data analysis is required to identify data cleaning necessities. Following are the steps to tidy the data:


#### Impute missing values

To prepare the dataset for analysis, getting rid if missing values in an important step. The method is by replacing the missing values with the mean for numerical variables; however, it makes more sense to remove records with missing values for categorical attribtes using the following code:


```{r impute_missing}
    # first to convert missing values ? to NA
    # for workclass, occupation, and native_country
    census_data$workclass<-ifelse(census_data$workclass=='?',
                                  NA, as.character(census_data$workclass))
    census_data$occupation<-ifelse(census_data$occupation=='?',
                                   NA, as.character(census_data$occupation))
    census_data$native_country<-ifelse(census_data$native_country=='?',
                                       NA, as.character(census_data$native_country))

    # next remove rows with missing values
    census_data <- na.omit(census_data)
    
```

<br/>

#### Income column 

Next, to aid the analysis, `income` column values can be changed to 1 and 0 respectively for greater than $50k and less then or equal to $50k income.

```{r income}
    # remove leading space in income factor using mutate function
    census_data <- census_data %>%
      mutate(income = factor(if_else(income == "<=50K", 0, 1)))
```


#### Education and Education_Num columns

`education` and `education_num` variables represents the same data with `education_num` as ordinal representation. We can get rid of `education_num` for cleaning the data using the following code:
 

```{r education_num}
    # to remove education_num column
    census_data <- census_data %>%
      select(-education_num)
```


<br/>

#### Capital Gain and Capital Loss columns

Next, the `capital_gain` and  `capital_loss` columns can be combined into a single colum `capital`. Further, `capital_gain` and  `capital_loss` columns can be removed to aid the analysis, using the following code:


```{r capital}
    # to combine capital gain and capital loss columns
    # into a single column Capital,
    # a positive value represents gain and negative represents loss
    census_data <- census_data %>%
      mutate(capital = capital_gain - capital_loss) %>%
      # then remove capital_gain and capital_loss
      select(-capital_gain, -capital_loss)
```


<br/>

#### Final Weight column

The final weight is represented by `fnlwgt` column, adjusts the weight as per population size of number of people in the census data.For example, a record gets a high weight if the proportion of such samples is comparatively small in the overall population and vice-versa. This is not useful in the analysis, hence, can be removed.


```{r fnlwgt}
    # to remove fnlwgt column
    census_data <- census_data %>%
      select(-fnlwgt)
```

#### Combine Marital Status column catagories

Next, cobmine `marital_status` column values into 3 categories - Married, Not-married, Never-married using the following code:


```{r marital_status}
    # to combine the column values catagories 
    census_data$marital_status[census_data$marital_status == "Married-AF-spouse" |
                                 census_data$marital_status == "Married-civ-spouse" |
                                 census_data$marital_status == "Married-spouse-absent"] <-
      "Married"
    
    census_data$marital_status[census_data$marital_status == "Divorced" |
                                 census_data$marital_status == "Separated" |
                                 census_data$marital_status == "Widowed"] <-
      "Not-married"
    
    # to display the table
    table(census_data$marital_status) %>%
      kable() %>%
      kable_styling(latex_options = c("striped", "hover", "condensed"))
```


#### Combine Native Country column catagories

There are too many categories in `native_country` column, it can be reduced to their regions and key countries.
The countries are categorized among their respective regions and leaving key countries such as the US to see if native to the countries make any difference to the income prediction.

```{r native_country}
    # to combine the column values catagories 

    LatinAmerica <- c("Dominican-Republic","Guatemala","Haiti","Honduras",
                      "Jamaica","Mexico","Nicaragua", "Outlying-US(Guam-USVI-etc)", 
                      "Puerto-Rico","Trinadad&Tobago","Cuba")

    SouthAmerica <- c("Peru","Ecuador","El-Salvador","Columbia")
    
    Europe <- c("France","Germany","Greece","Holand-Netherlands",
                "Hungary","Italy","Poland", "Portugal","Puerto-Rico",
                "South","Ireland","Yugoslavia")
    
    Asia <- c("China","Hong","India","Japan","Iran")
    
    SE_Aisa <- c("Vietnam","Cambodia","Thailand","Laos","Philippines","Taiwan")
    
    US <- c("United-States")
    UK <- c("England","Scotland")
    Canada <- c("Canada")

    # to update the native_country column with the new catagories    
    census_data$native_country[census_data$native_country %in% LatinAmerica] <-
      "Latin America"
    census_data$native_country[census_data$native_country %in% Asia] <-
      "Asia"
    census_data$native_country[census_data$native_country %in% SE_Aisa] <-
      "South East Aisa"
    census_data$native_country[census_data$native_country %in% SouthAmerica] <-
      "South America"
    census_data$native_country[census_data$native_country %in% Europe] <-
      "Europe"
    census_data$native_country[census_data$native_country %in% US] <-
      "US"
    census_data$native_country[census_data$native_country %in% UK] <-
      "UK"
    census_data$native_country[census_data$native_country %in% Canada] <-
      "Canada"

    # to display the data in a tabular form
    table(census_data$native_country) %>%
          kable() %>%
          kable_styling(latex_options = c("striped", "hover", "condensed"))
```

<br/>

#### Combine Education column catagories

Similarly, categories in `education` column can be combined to make modelling efficient.


```{r education}
    # to combine the column values catagories 

    census_data$education = gsub("^10th", "No-college", census_data$education)
    census_data$education = gsub("^11th", "No-college", census_data$education)
    census_data$education = gsub("^12th", "No-college", census_data$education)
    census_data$education = gsub("^1st-4th", "No-college", census_data$education)
    census_data$education = gsub("^5th-6th", "No-college", census_data$education)
    census_data$education = gsub("^7th-8th", "No-college", census_data$education)
    census_data$education = gsub("^9th", "No-college", census_data$education)
    census_data$education = gsub("^Assoc-acdm", "Associates", census_data$education)
    census_data$education = gsub("^Assoc-voc", "Associates", census_data$education)
    census_data$education = gsub("^Bachelors", "Bachelors", census_data$education)
    census_data$education = gsub("^Doctorate", "Doctorate", census_data$education)
    census_data$education = gsub("^HS-Grad", "HS-Graduate", census_data$education)
    census_data$education = gsub("^Masters", "Masters", census_data$education)
    census_data$education = gsub("^Preschool", "No-college", census_data$education)
    census_data$education = gsub("^Prof-school", "Prof-School", census_data$education)
    census_data$education = gsub("^Some-college", "Some-college", census_data$education)
    
        # to display the data in a tabular form
    table(census_data$education) %>%
          kable() %>%
          kable_styling(latex_options = c("striped", "hover", "condensed"))

```
Please note: There is a weird space "^" is observed in education column values, so *gsub* is used to do string replace for values.

#### Combine Workclass column catagories

Next, categories in `workclass` column can be combined into fewer catogories as follows:


```{r workclass}
    # to combine the column values catagories 

    census_data$workclass = gsub("^Without-pay", "Not-Working", census_data$workclass)
    census_data$workclass = gsub("^Never-worked", "Not-Working", census_data$workclass)
    census_data$workclass = gsub("^Self-emp-not-inc", "Self-Employed", census_data$workclass)
    census_data$workclass = gsub("^Self-emp-inc", "Self-Employed", census_data$workclass)
    
        # to display the data in a tabular form
    table(census_data$workclass) %>%
          kable() %>%
          kable_styling(latex_options = c("striped", "hover", "condensed"))

```



\pagebreak

# 4. Exploratory Data Analysis and Visualization

The next step is to perform exploratory data analysis on the tidy data by utilizing visualization techniques.

#### Ratings Distribution

The chart shows that users have a preference to rate movies rather higher than lower. 4.0 is the most common rating, followed by 3.0 and 5.0. 0.5 is the least preferable rating and there is no movie with a 0.0 rating.

The difference in the median and mean noted in the *Summary and Average Ratings* section above also supports the negative skewness slope of the distribution towards higher ratings.

```{r rating_distribution}
  
```


### 2.3 Spliting dataset: Training and Validation Sets

The census_data dataset will be split into *census_data_train* and *census_data_test* set (20%). The algorithm is developed using the census_data_train and for a final test of the algorithm, income will be predicted on the census_data_test set as if they were unknown. 

\pagebreak

The following code is used to generate the required datasets. 

```{r split datasets, cache = TRUE}
    ##############################################################################    
    # to split datasets into edx, validation, census_data_train, and test_set
    ##############################################################################
set.seed(1)
# Partition the data set into census_data_train and census_data_test dataset
# with respect to dependent variable income
# The census_data_test set will be 10% of census_data
test_index <- createDataPartition(census_data$income,
                                  times = 1,
                                  p = 0.8,
                                  list = FALSE)
census_data_train <- census_data[test_index, ]
census_data_test  <- census_data[-test_index, ]
# Remove unused objects from the memory
rm(test_index)
```


<br/>

\pagebreak

# 5. Modeling Approach

Now, with the analysis we are ready for modeling the prediction model, this section describes modeling approaches and insights gained before finalizing a target model.

### 5.1 Model 1: Logistic Regression Model

```{r log_model, eval= FALSE}
    # to create logistic regression model
    log_model <- glm(income ~ ., data = census_data_train, family = "binomial")
    summary(log_model)
    
    
    predicted <- predict(log_model, newdata = census_data_test, type = "response")
    predicted_class <- ifelse(predicted > 0.5, "gt50K", "lte50K")
    proportions <- addmargins(table(census_data_test$income, predicted_class))
    caret::confusionMatrix(predicted_class, census_data_test$income, positive = "gt50K")
    #Accuracy is 0.8524 on testing set.
```


```{r Accuracy_results, eval= FALSE}
      # to display Model Accuracy results in a tabular form
      model_results <- tibble(Model = "Logistic Regression Model", 
                             Dataset = "census_data_test", Accuracy = round(1.222222, digits = 5))
      model_results %>% 
      
      # to apply theme to the table
      kable("latex", booktabs = T) %>% 
      kable_styling(latex_options = c("striped", "hover", "condensed")) %>%
      
      row_spec(1:1, bold = T, color = "white", background = "#D7261E")
```



### 5.2 Model 1: Basic Rating Mean Model 

Let's start with a very basic model based on the rating average. This simplest possible prediction model will predict the same rating for all movies without taking any other predictors into account. The difference in the predicted values is explained by random variation in the independent error variable.

The model can be represented as follows:

$$ Y_{u, i} = \mu + \epsilon_{u, i} $$

where $\mu$ is the mean which shows the true rating value for all movies. And, $\epsilon_{u,i}$ is the independent random error variable.

Now, lets now calculate RMSE for this model as follow:


```{r rmse_mu}
```

This gives us our first model with RMSE to start with. Also, it can be noticed that this is equivalent to the standard deviation of rating distribution. 

The results can be displayed in a table for easy comparison using the following code:

```{r RMSE_result_tab}
```

We can further refine this basic prediction model by removing outlier data points from the calculation; however, this may not yield the desired RMSE improvement. So we will move to the predictor based models by utilizing potential predicators insights gained during exploratory data analysis.


<br/>


### 5.3 Single Predictor Model

To begin with a more complex model, we can start with a single potential predictor identified during the data exploration. Each potential predictors can be evaluated and compared with other models based on the RMSE generated.

#### 5.3.1 Model 2: Movie Effect Model

This model takes into account the effect of the movie considering that each movie is not rated equally and introduces a movie bias term to the *Basic Rating Mean Model*. The term is based on individual movie mean rating and its difference with the overall mean rating. 

As it can be seen from the graph below, the left skewness implies that more movies have negative effects (rated less than the average).

```{r movie bias, fig.align='center',echo = FALSE}  
```

The model can be denoted as follow:

$$ Y_{u,i} = \mu + b_i + \epsilon_{u,i} $$
where $Y_{u,i}$ is the predicted rating, $\epsilon_{u,i}$ is the independent error, $\mu$ the mean rating for all movies, and $b_i$ is the bias for each movie $i$: 

The least-squares lm() method is not being used as it is computation-intensive and slow on a large datasets, specifically computing bias for each movie. So we will use the average of the residuals.

Individual movie bias is calculated using the formula: $$b_i = mean (rating_i - \mu)$$


<br/>




<br/>

# 7. Conclusion
### 7.1 Brief summary

The main aim of the project is to develop a recommendation system to predict movie ratings. The provided 10M Adult Census Income dataset is a real-world challenge that seasoned data scientists would have handled. From data setup, data wrangling as well as exploratory analysis and ggplot based visualization graphs to various modeling approaches, the concepts learned throughout the series of courses are implemented. 

Initially, basic prediction models including Rating Mean and single predictors based models are developed, later more complex multiple predictors models are attempted and finally **regularized Movie and User Effect predictive model** is implemented. A RMSE value of **`r ` ** is attained on validation set which is less than the targeted value.
 
### 7.2 Future work
The model using the Movie and User effects seems to have the best performance among the other two predictor models; however, multiple combinations of more than two potential predictors can be explored for further improvements as future work. 
For example, *Genre-specific* effect on a user preference to rate a movie can be further explored. If a user rates movies with Drama genre high, the *Drama* effect for that user can be taken into account while predicting rating as the user is more likely to give higher ratings to movies with Drama as a sub-genre in combined genre Drama|Comedy and vice-versa.
Such a genre-specific effect can be used along with Movie and User Effects for further improvement. 
Furthermore, Other more complex modeling approaches such as XGBoost, NNET, etc could be used to further improve the performance.
### 7.3 Limitations 
Initially, k-fold cross-validation with k=10 was planned; however, due to computing resource constraint it was not successful. So, a simpler alternative approach to partition provided edx dataset into training and test sets are utilized.
Also, due to the constraint, more sophisticated models with higher performance didn't succeed and thus not included in this report. Those would require powerful computing machines. 
One such example is splitting single pipe-delimited concatenated `genres` categories into single categories and use it in genre-specific effect, mentioned in '*Future Work* section'. However, the target RMSE was achieved with the Regularized Movie and User Effect model implemented in the project.
<br/>
\pagebreak

# 8. References
* Data Science textbook by Rafael Irizarry
    + [Chapter 34.7 Recommendation systems](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems)
    + [Chapter 34.9 Regularization](https://rafalab.github.io/dsbook/large-datasets.html#regularization)
* UCI
    + Adult Census Income Dataset: http://archive.ics.uci.edu/ml/machine-learning-databases/adult

# 9. Github Repo
* https://github.com/jha-r/Harvardx-PH125.9x-Capstone-Census_Income_CYO
```{r memory cleanup, echo = FALSE, results = "hide"}
      # Remove objects    
      #rm()  
      # Call Garbage Collector
      gc()
```