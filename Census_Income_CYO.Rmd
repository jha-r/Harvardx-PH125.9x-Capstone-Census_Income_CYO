---
title: "HavardX PH125.9x Data Science Capstone CYO Project Report"
author: "Ravi Jha"
date: "12/20/2019"
output:
  html_document: default
  pdf_document:
    df_print: kable
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
    library(knitr)
    knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                          cache.lazy = FALSE, fig.align='center', fig.width=5.5, fig.height=3.5)
```

\pagebreak

# 1. Introduction

### 1.1 Overview

This report is prepared to fulfill the completion requirement of the **HavardX PH125.9x Data Science Capstone** course. 
TBD

model to classify people using demographics to predict whether a person will have an annual income over 50K dollars or not.

### 1.2 Motivation
TBD

### 1.3 Project Goal

The goal of this project is to train machine learning algorithms that predicts whether income exceeds $50K/yr based on census data and then compare their performance.


### 1.4 Key steps performed

Following are the key steps performed in this project:

 * Importing the dataset and setup
 * Spliting datasets
 * Data wrangling
 * Exploratory data analysis and visualization 
 * Creating and tuning predective models
 * Evaluating models based on validation dataset
 * Reporting results 

\pagebreak

# 2. Data and setup

The first step in the whole process is to import and setup data including splitting it in appropriate sub-datasets. 

### 2.1 Adult Census Income DataSet

Let's start with the dataset, as per course instruction, Machine Learning analyses friendly **Adult Census Income** dataset, from UCI's curated list of datasets, is used. The data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics). The dataset is then cleaned considering the following criteria: 

 - age > 16
 - adjusted gross income > $100
 - final weighting > 1
 - working hours per week > 0

This data set can be found and downloaded here:

* Adult Census Income dataset:           http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data


### 2.2 Data Import and Initial Setup

To assist with our data import and initial setup, several packages from *CRAN* is utilized and loaded. These will be automatically downloaded and installed during code execution. 


```{r data import and initial setup}
##########################
# to import and setup data
##########################

# Install requested packages if not found
if (!require(tidyverse))
  install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if (!require(caret))
  install.packages("caret", repos = "http://cran.us.r-project.org")
if (!require(data.table))
  install.packages("data.table", repos = "http://cran.us.r-project.org")
if (!require(dplyr))
  install.packages("dplyr")
if (!require(tidyr))
  install.packages("tidyr")


# Load libraries
library(tidyverse)
library(caret)
library(hexbin)
library(lubridate)
library(knitr)
library(kableExtra)

# Adult Census Income dataset
# http://archive.ics.uci.edu/ml/machine-learning-databases/adult

datafile <- "Income.RData"

# Check if datafile is already downloaded
if (!file.exists("Income.RData"))
{
  fileURL <-
    "http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
  
  destfile_adult <- "./dataset/adult.data"
  
  # Download only if destination data file - adult.data
  # is not found on the dataset directory folder
  if (!file.exists(destfile_adult))
  {
    # Create dataset folder file if it doesn't exists
    if (!dir.exists("./dataset"))
    {
      dir.create("./dataset")
    }
    
    # Download file from url to temp file
    download.file(fileURL, destfile_adult,  quiet = TRUE)
    print("Downloading file to the dataset directory")
    
  }
  
  c_names <-
    c(
      "age", "workclass", "fnlwgt", "education", "education-num", 
      "marital-status", "occupation", "relationship", "race", "sex", 
      "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"
    )
  
  
  # read the data from the file and store in the census_data object
  census_data <-
    read.csv(file = "./dataset/adult.data", header = FALSE, 
             col.names = c_names, stringsAsFactors = T, strip.white = TRUE)
  
  # replace . in the columns names
  colnames(census_data) <- str_replace_all(colnames(census_data), "\\.", "_")
  
  # remove leading space in income factor using mutate function
  census_data <- census_data %>%
    mutate(income = factor(if_else(income == " <=50K", "lte50K", "gt50K")))
  
  # Save census_data object to datafile
  save(census_data, file = datafile)
  
  # Remove unused objects from the memory
  rm(c_names, fileURL, datafile, destfile_adult)
  
} else {
  # Load the datafile if it already exists
  load(datafile)
}

```


<br/>

### 2.3 Spliting dataset: Training and Validation Sets

The census_data dataset will be split into *census_data_train* and *census_data_test* set (20%). The algorithm is developed using the census_data_train and for a final test of the algorithm, income will be predicted on the census_data_test set as if they were unknown. 

\pagebreak

The following code is used to generate the required datasets. 

```{r split datasets, cache = TRUE}
    ##############################################################################    
    # to split datasets into edx, validation, census_data_train, and test_set
    ##############################################################################

set.seed(1)

# Partition the data set into census_data_train and census_data_test dataset
# with respect to dependent variable income
# The census_data_test set will be 10% of census_data
test_index <- createDataPartition(census_data$income,
                                  times = 1,
                                  p = 0.8,
                                  list = FALSE)

census_data_train <- census_data[test_index, ]
census_data_test  <- census_data[-test_index, ]

# Remove unused objects from the memory
rm(test_index)

```


<br/>

\pagebreak

# 3. Data Wrangling

The next step is to understand the data through a brief data review and then do data wrangling as per the necessity.

### 3.1 Initial exploration

Before proceeding further with analysis, examining the data is important. The first step in it is to understand the format and the contents by looking at the few rows.

```{r Inital analysis-head}
    # Display first few rows to understand the data structure
    head(census_data, 5)%>%
    
    # to format table with theme
    kable() %>% 
    kable_styling(latex_options = c("striped", "hover", "condensed", "scale_down"))
```


<br/>

The dataset has 32,561 records and 15 attributes

```{r row_columns class, echo=FALSE}
    
    # to display class of userId and movieId column in a tabular form
      # to display number of rows and columns in a tabular form
      row_cols <- tibble(Rows = nrow(census_data), 
                             Columns = ncol(census_data))
      row_cols %>% 
      
      # to apply theme to the table
      kable() %>% 
      kable_styling(latex_options = c("striped", "hover", "condensed"))
``` 

<br/>


#### Dataset Summary 

```{r summary, include=TRUE}
    # Summary of the census_data_train dataset
    summary(census_data)%>%
    
    kable() %>% 
    kable_styling(latex_options = c("striped", "hover", "condensed", "scale_down"))
```

<br/>


#### Column types and Summary

Let's check column types, their class, and summary


```{r Columns}
    # To display dataset column types and summary

    col_type <- as.data.frame(lapply(census_data, class)) %>%
      .[1, ] %>%
      gather(variable, R_class , 1:ncol(.)) %>%
      mutate(data_type = ifelse(
        R_class == "factor",
        "categorical",
        ifelse(variable == "education_num", "categorical", "continuous")
      )) %>%
      
      # to format table with theme
      kable() %>%
      kable_styling(latex_options = c("striped", "hover", "condensed")) 
```


<br/>

#### Duplicate Records

Let's first check for any duplicate record. The following table shows there are 24 duplicate rows.

```{r duplicates}
 
```


<br/>

#### Missing Data

By closely looking at the data, it can be observed that there are columns with value "?". This is also supported by the dataset details available on UCI wesite that **unknown** values are labelled as **?**. 

As it can be seen from the below table, there are 2,399 incomplete records (~7.4%). Specifically, there are missing values in occupation, workclass and native_country, which accounts for 5.7%, 5.6% and 1.8% of the total records respectively.

```{r missing data}

    #to check if any missing value marked with NA
      anyNA(census_data)
    
    # to check missing values labeled as "?"
    total_missing <-
      map_df(census_data, ~ str_detect(., pattern = "\\?")) %>%
      rowSums() %>%
      tbl_df() %>%
      filter(value > 1) %>%
      summarize(no_of_missing = n()) %>%
      mutate(pct_missing = round(no_of_missing / nrow(census_data) * 100, 1),
             variable = "all variables") %>%
      select(variable, no_of_missing, pct_missing)
    
    
    map_df(census_data, ~ sum(str_detect(., pattern = "\\?"))) %>%
      gather(variable, no_of_missing, 1:15) %>%
      mutate(pct_missing = round(no_of_missing / nrow(census_data) * 100, 1)) %>%
      bind_rows(total_missing) %>%
      arrange(desc(no_of_missing)) %>%
      # to format table with theme
      kable() %>%
      kable_styling(latex_options = c("striped", "hover", "condensed")) 

```


<br/>


<br/>

#### Factorizing Variables

In the dataset, columns `userId` and `movieId` contains numeric data and treated as numeric/integer by R. These columns can be converted to factor as this could be used as categorical data; however, in the current scenario it makes more sense to leave it as numeric to keep later computation easier e.g. summarize() function and other numeric calculations can be done easily.

```{r userid-movieid class, echo=FALSE}
    

``` 


<br/>

#### Label encoding
There are no columns with content as ordinal labels, so label encoding is not required in the dataset.


<br/>

### 3.2 Tidying data

Based on the initial understanding further data analysis is required to identify data cleaning necessities. Following are the steps to tidy the data:


TBD
Income
education
weight
....

<br/>

#### Extracting Movie Release Year
 
The `title` column includes movie title and year the movie was released. This can be extracted as `year_released` in a separate column using the following code:

```{r extract-year_released}

```


<br/>

#### Date Rated and Year Rated - Converting 'timestamp' to date

Next, the `timestamp` column contains a UNIX timestamp (the number of seconds since January 1, 1970), which can be converted to easy to understand date and extracted as a `date_rated` separate column.

Further, `year_rated` can be extracted into a separate column to aid the analysis, using the following code:


```{r date-year rated}

```


<br/>

#### Movie Current Age and Age at the time of Review

Other columns such as `movie_age` (the current age of the movie) and `age_at_review` (age of the movie when it was rated) can be helpful for the analysis. Following is the code to add it in the dataset:


```{r movie_age and age_at_review}
    

```



<br/>
\pagebreak

# 4. Exploratory Data Analysis and Visualization

The next step is to perform exploratory data analysis on the tidy data by utilizing visualization techniques.

#### Ratings Distribution

The chart shows that users have a preference to rate movies rather higher than lower. 4.0 is the most common rating, followed by 3.0 and 5.0. 0.5 is the least preferable rating and there is no movie with a 0.0 rating.

The difference in the median and mean noted in the *Summary and Average Ratings* section above also supports the negative skewness slope of the distribution towards higher ratings.

```{r rating_distribution}

  
```


<br/>

\pagebreak


<br/>

\pagebreak

# 5. Modeling Approach

Now, with the analysis we are ready for modeling the prediction model, this section describes modeling approaches and insights gained before finalizing a target model.

### 5.1 Loss Function

As discussed earlier we will use *Root Mean Square Error*, or *RMSE* loss function to measure the performance of the models. Lower the RMSE better the performance of the model.


$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$

where N is sample size, y is the observed value, $\hat{y}$ is predicted value for u,i times and $\sum$ = summation 
 

### 5.2 Model 1: Basic Rating Mean Model 

Let's start with a very basic model based on the rating average. This simplest possible prediction model will predict the same rating for all movies without taking any other predictors into account. The difference in the predicted values is explained by random variation in the independent error variable.

The model can be represented as follows:

$$ Y_{u, i} = \mu + \epsilon_{u, i} $$

where $\mu$ is the mean which shows the true rating value for all movies. And, $\epsilon_{u,i}$ is the independent random error variable.

Now, lets now calculate RMSE for this model as follow:


```{r rmse_mu}

```

This gives us our first model with RMSE to start with. Also, it can be noticed that this is equivalent to the standard deviation of rating distribution. 

The results can be displayed in a table for easy comparison using the following code:

```{r RMSE_result_tab}

```

We can further refine this basic prediction model by removing outlier data points from the calculation; however, this may not yield the desired RMSE improvement. So we will move to the predictor based models by utilizing potential predicators insights gained during exploratory data analysis.


<br/>


### 5.3 Single Predictor Model

To begin with a more complex model, we can start with a single potential predictor identified during the data exploration. Each potential predictors can be evaluated and compared with other models based on the RMSE generated.

#### 5.3.1 Model 2: Movie Effect Model

This model takes into account the effect of the movie considering that each movie is not rated equally and introduces a movie bias term to the *Basic Rating Mean Model*. The term is based on individual movie mean rating and its difference with the overall mean rating. 

As it can be seen from the graph below, the left skewness implies that more movies have negative effects (rated less than the average).

```{r movie bias, fig.align='center',echo = FALSE}  

```

The model can be denoted as follow:

$$ Y_{u,i} = \mu + b_i + \epsilon_{u,i} $$
where $Y_{u,i}$ is the predicted rating, $\epsilon_{u,i}$ is the independent error, $\mu$ the mean rating for all movies, and $b_i$ is the bias for each movie $i$: 

The least-squares lm() method is not being used as it is computation-intensive and slow on a large datasets, specifically computing bias for each movie. So we will use the average of the residuals.

Individual movie bias is calculated using the formula: $$b_i = mean (rating_i - \mu)$$


<br/>




<br/>

# 7. Conclusion
### 7.1 Brief summary

The main aim of the project is to develop a recommendation system to predict movie ratings. The provided 10M Adult Census Income dataset is a real-world challenge that seasoned data scientists would have handled. From data setup, data wrangling as well as exploratory analysis and ggplot based visualization graphs to various modeling approaches, the concepts learned throughout the series of courses are implemented. 

Initially, basic prediction models including Rating Mean and single predictors based models are developed, later more complex multiple predictors models are attempted and finally **regularized Movie and User Effect predictive model** is implemented. A RMSE value of **`r ` ** is attained on validation set which is less than the targeted value.
 

### 7.2 Future work

The model using the Movie and User effects seems to have the best performance among the other two predictor models; however, multiple combinations of more than two potential predictors can be explored for further improvements as future work. 

For example, *Genre-specific* effect on a user preference to rate a movie can be further explored. If a user rates movies with Drama genre high, the *Drama* effect for that user can be taken into account while predicting rating as the user is more likely to give higher ratings to movies with Drama as a sub-genre in combined genre Drama|Comedy and vice-versa.
Such a genre-specific effect can be used along with Movie and User Effects for further improvement. 

Furthermore, Other more complex modeling approaches such as XGBoost, NNET, etc could be used to further improve the performance.

### 7.3 Limitations 

Initially, k-fold cross-validation with k=10 was planned; however, due to computing resource constraint it was not successful. So, a simpler alternative approach to partition provided edx dataset into training and test sets are utilized.

Also, due to the constraint, more sophisticated models with higher performance didn't succeed and thus not included in this report. Those would require powerful computing machines. 

One such example is splitting single pipe-delimited concatenated `genres` categories into single categories and use it in genre-specific effect, mentioned in '*Future Work* section'. However, the target RMSE was achieved with the Regularized Movie and User Effect model implemented in the project.

<br/>
\pagebreak

# 8. References

* Data Science textbook by Rafael Irizarry
    + [Chapter 34.7 Recommendation systems](https://rafalab.github.io/dsbook/large-datasets.html#recommendation-systems)
    + [Chapter 34.9 Regularization](https://rafalab.github.io/dsbook/large-datasets.html#regularization)

* UCI
    + Adult Census Income Dataset: http://archive.ics.uci.edu/ml/machine-learning-databases/adult


# 9. Github Repo

* https://github.com/jha-r/Harvardx-PH125.9x-Capstone-Census_Income_CYO

```{r memory cleanup, echo = FALSE, results = "hide"}
      # Remove objects    
      #rm()  
      # Call Garbage Collector
      gc()
```
